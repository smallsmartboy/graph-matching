\section{Related Work}

Recommendation systems have been studied extensively in the literature, %especially since the advent of the web. Most recommendation systems can be
broadly separated into two different streams: collaborative filtering systems and content-based recommender systems \cite{almazro2010survey}. Much attention has been focused on the former approach, where either users are clustered by considering the items they have consumed or items are clustered by considering the users that have bought them. Both item-to-item and user-to-user recommendation systems based on collaborative filtering have been adopted by many industry giants such as Twitter, % \cite{twitter-collab-filtering}
Amazon, %\cite{amazon-collab-filtering} 
and Google %\cite{google-collab-filtering}.  
 

Content based systems instead look at each item and its intrinsic properties. For example, Pandora has categorical information such as Artist, Genre, Year, Singer, Tempo etc. on each song it indexes. This categorical data can then be used to recommend new songs that are similar to the songs that a user has liked before. Depending on user feedback, a recommender system can learn which of the categories are more or less important to a user and adjust its recommendations.  

Neither system is ideal for e-commerce sites, where the engagement isn't hight enough to use collaborative filtering, and categorical data is hard to collect. Because of these constraints, in this paper we focus on a recommender system that algorithmically extracts categorical data from item descriptions and uses this data to establish weak links between items (candidate recommendations). In the absence of other data that would enable us to choose among these many links, we consider every potential recommendation to be of equal value and focus on the objective of discovery, which has not been studied before. In this way, our work differs from all the previous work on recommendation systems that emphasize on finding recommendations of high relevance and quality rather than on structural navigability of the realized link structure. However, while it's not included in this paper for brevity, some of our approaches can be extended to the more general case where different recommendations have different weights.  

On the graph algorithms side, our problem is related to the bipartite matching and more generally, the maximum $b$-matching problems. There has been considerable work done in this area.
 %when it comes to approximation algorithms. 
In particular, both the weighted matching and $b$-matching problems have exact polynomial time solutions ~\cite{Gabow1983}. Furthermore the matching problem admits a near linear time $(1-\epsilon)$-approximation algorithm~\cite{duan2010approximating}, while the weighted $b$-matching problem admits a $1/2$-approximation algorithm ~\cite{koufogiannakis2009distributed}. However, all such algorithms are based on combinatorial properties of matchings and $b$-matchings, and do not carry over to the more important version of our problem when $a > 1$.

Finally, our problem bears resemblance to some covering problems. For example, the maximum coverage problem asks for the maximum number of elements that can be covered by a fixed number of sets and has a greedy $(1-1/e)$-approximation ~\cite{nemhauser1978analysis}. However, as mentioned earlier, our formulation requires multiple coverage of elements. Furthermore note that the collection of sets that can be used in the redundant coverage are all possible subsets of $c$ out of the $d$ candidate links, and is expressed implicitly in our problem.  The currently known theoretical methods for maximum coverage heavily rely on the submodularity of the objective function, which our objective doesn't satisfy. Hence the line of recent work on approximation algorithms for submodular maximization does not apply to our problems. 