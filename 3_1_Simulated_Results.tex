\section{Experimental Results}

******I just copied from the old paper below just to remind ourselves of what plots we need to generate: Needs work.


\subsection{Simulated Runs}
We simulated performance of our algorithms on random graphs generated
by the graph models we outlined. In the following figures, each data
point is obtained by averaging the measurements over 10 random
graphs,  where the red, blue and green lines denote the approximation
ratio of the greedy, sampling and partitioning algorithms respectively.
% **** Write two sentences about the partition algorithm here: What do you do after partitioning? Solve matching optimally?
Recall that the partition algorithm split the graph into multiple graphs and found
matchings in these smaller graphs which were then combined into a
recommendation subgraph. For this reason, a run of the partition
algorithm takes much longer to solve a problem instance than either the
sampling or greedy algorithms.




**** Do the figures below only lower bounds, or actual performances as a percentage of the size of $|R|$?


Figures~\ref{fig:a=1:1} and \ref{fig:a=1:2} show that the
lower bound we calculated for the expected performance of the sampling
algorithm accurately captures the behavior of the sampling algorithm
when $a=1$. Indeed, the inequality we used is an accurate
approximation of the expectation, up to lower order terms. The random
sampling algorithm does well, both when $c$ is low and high, but
falters when $ck=1$. The greedy algorithm performs better than the
random sampling algorithm in all cases, but its advantage vanishes as
$c$ gets larger. Note that the dip in the graphs when $cl=ar$, at
$c=4$ in Figure~\ref{fig:a=1:1} and $c=2$ in Figure~\ref{fig:a=1:2} is
expected and was previously demonstrated in Figure~\ref{fig:simple_approx}.

\begin{figure}[t]
\centering
\begin{minipage}[h]{0.48\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{images/l=25000,r=100000_Greedy_vs_Naive.png}
\caption{$|L|=25$k, $|R|=100$k, $d=20$, $a=1$}\label{fig:a=1:1}
\end{minipage}
\hspace{0cm}
\begin{minipage}[h]{0.48\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{images/l=50000,r=100000_Greedy_vs_Naive.png}
\caption{$|L|=50$k, $|R|=100$k, $d=20$, $a=1$}\label{fig:a=1:2}
\end{minipage}
\vspace{-0.2in}
\end{figure}


% Can we show the graph for larger c? Maybe up to 20?
% Also I can't believe that greedy is optimal. I am still tempted to say that
% greedy will do badly when d is small. Like d < 5.

In contrast to the case when $a=1$, the sampling algorithm performs
worse when $a>1$ but performs increasingly better with $c$ as
demonstrated by Figures~\ref{fig:a=2} and \ref{fig:a=4}. The greedy
algorithm continues to produce solutions that are nearly optimal,
regardless of the settings of $c$ and $a$. Therefore, our simulations
suggest that in many cases a software engineer can simply design the
sampling method for solving the $(c, a)$-recommendation subgraph
problem. In those cases where the sampling is not suitable as flagged by our analysis, we still
find that the greedy performs adequately and is simple to implement.

\begin{figure}[t]
\centering
\begin{minipage}[h]{0.48\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{images/l=50000,r=100000,a=2_Greedy_vs_Naive.png}
\caption{$|L|=50$k, $|R|=100$k, $d=20$, $a=2$}\label{fig:a=2}
\end{minipage}
\hspace{0cm}
\begin{minipage}[h]{0.48\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{images/l=50000,r=100000,a=4_Greedy_vs_Naive.png}
\caption{$|L|=50$k, $|R|=100$k, $d=20$, $a=4$}\label{fig:a=4}
\end{minipage}
\vspace{-0.2in}
\end{figure}


TODO: Run simulations for much larger sizes of $R$ say 1M, 10M and if possible 100M nodes. Show how far partition can go in these runs and when it is too slow and why.

TODO: Redo the same plots for three different client sets at three different scales: larger $R$, the better. Again, see if you can run Partition in any of them and when it chokes. 

TODO: While running on real data what is the scale for performance that we will be using? Will we just plot the number of nodes of $R$ that had degree at least $a$ in the solution without a benchmark for the size of optimal other than just $|R|$? Can we do anything interesting here?

TODO: Are there other statistics about these three methods that would be illustrative to plot? e.g. running time as we scale up $c$ and the size of $R$ for each of the three programs would be good to measure and plot.

 