\section{Our Model}
\label{modelsec}

We model the structure optimization of recommendations by using a bipartite
digraph, where one partition $L$ represents the set of discovered
(i.e., often visited) items for which we are required to suggest recommendations and the other partition $R$
representing the set of undiscovered (not visited) items that can be potentially recommended. If
needed, the same item can be represented in both $L$ and $R$.
\vs

\subsection{The Recommendation Subgraph Problem}
\label{recsub}
We introduce and study this as the {\bf the $(c, a)$-recommendation subgraph problem} in this paper:
{\em
 The input to the problem is the graph where each
$L$-vertex has $d$ recommendations. Given the space restrictions to
display recommendations, the output is a subgraph where each vertex in
$L$ has $c < d$ recommendations. The goal is to maximize the number of
vertices in $R$ that have in-degree at least a target integer $a$.
}

\vs

Note that if $a=c=1$ this is simply the maximum bipartite
matching problem~\cite{LovaszPlummer1986}. If $a=1$ and $c > 1$, we
obtain a $b$-matching problem, that can be converted to a bipartite
matching problem~\cite{Gabow1983}. The typical and interesting cases when $a > 1$ is most likely NP-hard, ruling out the possibility of eficient exact algorithms.\vs

We now describe typical web graph characteristics by discussing the
sizes of $L$, $R$, $c$ and $a$ in practice. As noted before, in most
websites, a small number of `head' pages contribute to a significant
amount of the traffic while a long tail of the remaining pages
contribute to the rest~\cite{HubermanAdamic1999,
  DuDemmerBrewer2006, KumarNorrisSun2009}. This is supported by our
own experience with the 80/20 rule, i.e. 80\% of a site's traffic is
captured by 20\% of the pages. Therefore, the ratio $k=|L|/|R|$ is
typically between $1/3$ to $1/5$, but may be even lower. \vs

From our own work at BloomReach (and by observing recommendations of Quora, Amazon, and YouTube), typical values for $c$ range from 3 to 20
recommendations per page. Values of $a$ are harder to nail down but it
typically ranges from $1$ to $5$.\vs

%The redundancy expressed by $a$
%reflects the amount of coverage required by web-crawlers for the page
%to include it in the searchable index of the crawler. \vs

\subsection{Practical Requirements}

There are two key requirements in making graph algorithms
practical. The first is that the method used must be very simple to
implement, debug, deploy and most importantly maintain long-term. The second is that the method must scale
gracefully with larger sizes. \vs

Graph matching algorithms require linear memory and super-linear run-time
which does not scale well. For example, an e-commerce website of a
client of BloomReach with 1M product pages and 100 recommendation
candidates per product would require easily over 160GB in main memory to store the graph
and run exact matching algorithms; this can be reduced by using graph
compression techniques but that adds more technical difficulties in
development and maintenance. Algorithms that are time intensive
can sometimes be sped-up by using distributed computing techniques such as
map-reduce~\cite{DeanGhemawat2004}. However, efficient map-reduce
algorithms for graph problems are notoriously difficult. Finally, all of these methods apply only to the special case of our problem when $a=1$, leaving open the question of solving the more interesting and typical cases of redundant coverage when $a > 1$.\vs

\subsection{Simple Approximation Algorithms}

To satisfy these practical requirements,
%the time and space complexity of implementing optimal
%graph algorithms for the recommendation subgraph problem,
we propose
the study of three simple approximate solutions strategies that not only can be shown to scale well in practice but also have good theoretical
properties that we demonstrate using approximation ratios.

\begin{itemize}

\item {\bf Sampling:} The first solution is a simple random sampling
  solution that selects a random subset of $c$ links out of the
  available $d$ from every page. Note that this solution requires no
  memory overhead to store these results a-priori and the
  recommendations can be generated using a random number generator on
  the fly. While this might seem trivial at first, for sufficient (and
  often real-world) values of $c$ and $a$ we show that this can be
  optimal. Also, this method is very easy to adapt to the case when the underlying graph is dynamic with both nodes and edges changing over time. Furthermore, our
  approach can be extended to the case where the recommendation edges
  have weights representing varying strengths of association as is typically provided by the traditional methods that generate candidate recommendation links\footnote{We omit a full description of this result for brevity.}.

\item {\bf Greedy:} The second solution we propose is a greedy
  algorithm that chooses the recommendation links so as to maximize
  the number of nodes in $R$ that can accumulate $a$ in-links. In
  particular, we keep track of the number of in-links required for
  each node in $R$ to reach the target of $a$ and choose the links
  from each node in $L$ giving preference to adding links to nodes in
  $R$ that are closer to the target in-degree $a$. This method bears close resemblance in strategy with greedy methods used for maximum coverage and its more general submodular maximization variants.

\item {\bf Partition:} The third solution is inspired by a
  theoretically rigorous method to find optimal subgraphs in
  sufficiently dense graphs: it partitions the edges into $a$ subsets
  by random sub-sampling, such that there is a good chance of finding
  a perfect matching from $L$ to $R$ in each of the subsets. The union
  of the matchings so found will thus result in most nodes in $R$
  achieving the target degree $a$. We require the number of edges in
  the underlying graph to be significantly large for this method to
  work very well; moreover, we need to run a (near-)perfect matching
  algorithm in each of the edge-subsets which is also a computationally
  expensive subroutine. Hence, even though this method works very well
  in dense graphs, its resource requirements may not scale well in terms of running
  time and space.
\end{itemize}

As a summary, the table below shows the time and space complexity
of our different algorithms.

\begin{figure}[H]
\centering
\begin{tabular}{l|l|l|l|l}
\cline{2-4}
                                    & Sampling                                        & Greedy   & Partition          &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Time}          & $O(|E|)$                                        & $O(|E|)$ & $O(|E|\sqrt{|V|})$ &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Working Space} & $O(1)$                                          & $O(V)$   & $O(|E|)$           &  \\ \cline{1-4}
%\multicolumn{1}{|l|}{Approx. Ratio} & $1-\frac{(ck)^a-1}{ck-1}e^{-ck+\frac{a-1}{r}}$  & $1-o(1)$ & $1-o(1)$           &  \\ \cline{1-4}
%\multicolumn{1}{|l|}{Is Optimal}    & NO                                              & NO       & YES                &  \\ \cline{1-4}
\end{tabular}
\caption{Complexities of the different algorithms (assuming constant $a$ and $c$)}
\end{figure}

In the next section, we elaborate on these methods, their running
times, implementation details, and theoretical performance
guarantees. In the section after that, we present our comprehensive
empirical evaluations of all three methods, first the results on
simulated data and then the results on real data from some clients of
BloomReach. 