\section{Our Model}

We model the structure optimization of recommendations by using a bipartite
digraph, where one partition $L$ represents the set of discovered (crawled or often visited) items for which
we are required to suggest recommendations and the other partition $R$
representing the set of undiscovered (uncrawled or not visited) items that can be potentially recommended. If
needed, the same item can be represented in both $L$ and $R$.
\vs

\subsection{The Recommendation Subgraph Problem}
We introduce and study this as the {\bf the $(c, a)$-recommendation subgraph problem} in this paper:
{\em
 The input to the problem is the graph where each
$L$-vertex has $d$ recommendations. Given the space restrictions to
display recommendations, the output is a subgraph where each vertex in
$L$ has $c < d$ recommendations. The goal is to maximize the number of
vertices in $R$ that have in-degree at least a target integer $a$.
}

\vs

Note that if $a=c=1$ this is simply the maximum bipartite
matching problem~\cite{LovaszPlummer1986}. If $a=1$ and $c > 1$, we
obtain a $b$-matching problem, that can be converted to a bipartite
matching problem~\cite{Gabow1983}.\vs

The requirement of having a minimum number of $a$ in-links from L for a node in R to be counted as discovered might seem unnatural at first sight. However, it is a natural formulation of the requirement of indexing algorithms for search engines where such a redundancy in their crawler visits is needed before recording pages in R in the search index and hence eventually as results for a relevant search query. Since most large web repositories using recommendations have a search capability, they also have such an indexing system that requires a minimum level of redundancy to include pages in their search index. Thus our problem formulation is also applicable in such general web systems that we study. \vs

We now describe typical web graph characteristics by discussing the
sizes of $L$, $R$, $c$ and $a$ in practice. As noted before, in most
websites, a small number of `head' pages contribute to a significant
amount of the traffic while a long tail of the remaining pages
contribute to the rest~\cite{HubermanAdamic1999,
  DuDemmerBrewer2006, KumarNorrisSun2009}. As demonstrated by a prior
measurement. This is supported by our
own experience with the 80/20 rule, i.e. 80\% of a site's traffic is
captured by 20\% of the pages. Therefore, the ratio $k=|L|/|R|$ is
typically between $1/3$ to $1/5$, but may be even lower. \vs

By observing recommendations of Quora, Amazon, YouTube and our own
work at BloomReach, typical values for $c$ range from 3 to 20
recommendations per page. Values of $a$ are harder to nail down but it
typically ranges from $1$ to $5$. The redundancy expressed by $a$
reflects the amount of coverage required by web-crawlers for the page
to include it in the searchable index of the crawler. \vs

\subsection{Practical Requirements}

Maintaining production software packages especially those involving algorithms
are extremely hard. There are two key requirements in making graph algorithms
practical. The first is that the method used must be very simple to
implement, debug, deploy and most importantly maintain long-term. The second is that the method must scale
gracefully with larger sizes. \vs

Graph matching algorithms require linear memory and super-linear run-time
which does not scale well. For example, an e-commerce website of a
client of BloomReach with 1M product pages and 100 recommendation
candidates per product would require easily over 160GB in main memory to store the graph
and run matching algorithms; this can be reduced by using graph
compression techniques but that adds more technical difficulties in
development and maintenance. Algorithms that are time intensive
can sometimes be sped-up by using distributed computing techniques such as
map-reduce~\cite{DeanGhemawat2004}. However, efficient map-reduce
algorithms for graph problems are notoriously difficult. \vs

\subsection{Simple Solutions}

To circumvent the time and space complexity of implementing optimal
graph algorithms for the recommendation subgraph problem, we propose
the study of three simple solutions strategies that not only can be
shown to scale well in practice but also have good theoretical
properties that we demonstrate using approximation ratios.

\begin{itemize}

\item {\bf Sampling:} The first solution is a simple random sampling
  solution that selects a random subset of $c$ links out of the
  available $d$ from every page. Note that this solution requires no
  memory overhead to store these results a-priori and the
  recommendations can be generated using a random number generator on
  the fly. While this might seem trivial at first, for sufficient (and
  often real-world) values of $c$ and $a$ we show that this can be
  optimal. Furthermore, while we omit this result for brevity, our 
  approach can be extended to the case where the recommendation edges
  have graphs.

\item {\bf Greedy:} The second solution we propose is a greedy
  algorithm that chooses the recommendation links so as to maximize
  the number of nodes in $R$ that can accumulate $a$ in-links. In
  particular, we keep track of the number of in-links required for
  each node in $R$ to reach the target of $a$ and choose the links
  from each node in $L$ giving preference to adding links to nodes in
  $R$ that are closer to the target in-degree $a$.

\item {\bf Partition:} The third solution is inspired by a
  theoretically rigorous method to find optimal subgraphs in
  sufficiently dense graphs: it partitions the edges into $a$ subsets
  by random sub-sampling, such that there is a good chance of finding
  a perfect matching from $L$ to $R$ in each of the subsets. The union
  of the matchings so found will thus result in most nodes in $R$
  achieving the target degree $a$. We require the number of edges in
  the underlying graph to be significantly large for this method to
  work very well; moreover, we need to run a (near-)perfect matching
  algorithm in each of the subsets which is also a computationally
  expensive subroutine. Hence, even though this method works very well
  in dense graphs, it does not scale very well in terms of running
  time and space.
\end{itemize}

In the next section, we elaborate on these methods, their running
times, implementation details, and theoretical performance
guarantees. In the following section, we present our comprehensive
empirical evaluations of all three methods, first the results on
simulated data and then the results on real data from some clients of
BloomReach.
