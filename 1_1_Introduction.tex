\section{Introduction}

\subsection{Web Relevance Engines}
Digital discovery divide (cite Bloomreach white paper) is the problem of companies not being able to present users with what they seek in the short time they spend looking for this information.
The problem is prevalent not only in e-commerce websites but also in social networks and micro-blogging sites where surfacing relevant content quickly is important for user engagement.
BloomReach is a big-data marketing company that uses the client's content as well as web-wide data to optimize both customer acquisition and satisfaction.
BloomReach's clients include popular retailers like Neiman Marcus, Crate \& Barrel, Williams-Sonoma and Staples besides many others. In this paper, we describe the structure optimizer component of the Web Relevance Engine.
This component works on top of recommendation engines so as to carefully add a set of links across pages that ensures that crawlers as well as users can efficiently navigate the entire website. 

\subsection{Structure Optimization of Websites}

One of the great benefits of the web as a useful source of hyperlinked
information comes from the careful choices made in crafting the
recommendations that link a page to closely related pages. Though this
advantage was identified well before the WWW was in place by
Bush~\cite{Bush45aswe}, it continues to persist even today.
Recent estimates~\cite{big-data-book13} attribute up to a third of the sales
on Amazon and three-quarters of new orders on Netflix are influenced by precisely choosing recommendations.

Recommendations exist across the entire web but we provide a couple of simple but concrete examples. First, YouTube has a section that displays all the related videos for every main video being viewed. 
Quora has a section for questions related to the main question that is displayed. These recommendations are critical in determining how the traffic across all of YouTube or Quora is going to flow.
As a website owner one simple yet big question that one can ask is if there is a significant fraction of the website that is not recommended at all (or `hardly' recommended) from other pages. Continuing
with the above example, if say half of the YouTube videos were recommended from other videos, then millions of great videos might be undiscovered for a long time. We try to address this
problem by trying to ensure that every page will require at least a baseline number of visits so that great content does not remain undiscovered. 

We use the criterion of discoverability as the objective for the choice of the links to recommend. We naturally get a new formulation of the recommendation selection problem that is structural. In particular, if we think of commonly visited pages in a site as the discovered pages, from which there are a large number of possible recommendations available to related but less visited peripheral pages, the problem of choosing a limited number of pages to recommend at each discovered page can be cast with the objective of maximizing the number of peripheral non-visited pages that are linked. We formulate this as a recommendation subgraph problem, and study practical algorithms for solving these problems in real-life data. 

\subsection{Recommendation Systems as a Subgraph Selection Problem}

Formally, we assume that recommendation systems~\cite{Schafer1999, Adomavicius2005,
  Resnick1997} find a large set of related candidate recommendations
for each page using relevance. In this work, we assume $d$
such related candidates are available per page creating a candidate recommendation graph.
Our goal is to analyze how to prune the set to $c < d$ recommendations such that
globally we ensure that the resulting recommendation subgraph can be navigated efficiently by the user to enable better discovery.

\subsection{Our Contributions}
While optimal solutions to some versions of the recommendation subgraph problem can be obtained by using a maximum matching algorithm, such algorithms are too costly to run on real-life instances. We introduce three simple alternate methods that can be implemented in linear or near-linear time and examine their properties. We show how these simpler methods perform very well on simulated data, and have very effective running times. Moreover, we define exactly when these
each method will work theoretically on random graph models and when a practioner needs to switch to a more sophisticated algorithm. We show the deployment of these methods on BloomReach's real-world client link graph and measure its
actual performance as well in terms of running-times, memory usage and accuracy.

To summarize, our contributions are the development of a new structural model for recommendation systems as a subgraph selection problem for maximizing discoverability, the proposal methods in increasing sophistication to solve them at scale along with some associated theoretical performance guarantee analysis for each method, and an empirical validation of our conclusions with simulated and real-life data.

