\section{Introduction}

\subsection{Web Relevance Engines}
Digital discovery divide (cite Bloomreach white paper) is the problem of companies not being able to present users with what they seek in the short time they spend looking for this information.

The problem is prevalent not only in eCommerce firms but also in social network and micro-blogging sites where surfacing relevant content quickly is important for user engagement.

Bloomreach has a web relevance engine to address the problem that is offered as a service to the websites of popular retailers like Neiman Marcus, Crate \& Barrel, Williams-Sonoma and Staples. Main idea is to optimize the site for relevance to the visitors: the site is thoroughly crawled and indexed for semantic interpretation and then eventual relevance scoring. The relevance scoring provides clues for optimizing the navigation structure of the sites that is done by a structure optimizer.

Structure optimizer solves the problem of establishing hyperlinks between the pages in the site for better discoverability. To quote the white-paper, the solution of the structure optimizer is a set of edges that ensure that each "non-crawled" page is easily accessible from a set of "crawled" pages.

\subsection{Structure Optimization of Websites}
Similar problems arise in all other recommendation systems where the set of possible relevant recommendations from each page is typically much larger than can be displayed as links in the page. This includes e.g., related videos in YouTube, relevant updates in FaceBook feeds, as well as related hashtags in a search for a hashtag in Twitter. All of these sites face the problem of choosing among very relevant content to surface.

If we use the criterion of discoverability as the objective for the choice of the links to recommend, we get a new formulation of the recommendation selection problem that is more structural: In particular, if we think of commonly visited and crawled pages in a site as the core pages, from which there are a large number of possible recommendations available to related but less visited peripheral pages, the problem of choosing a limited number of pages to recommend at each core page can be cast with the objective of maximizing the number of peripheral non-crawled pages that are linked. We formulate this as a recommendation subgraph problem, and study practical algorithms for solving these problems in real-life data.

\subsection{Recommendation Systems as a Subgraph Selection Problem}

(From previous writeup - needs to be integrated better)
One of the great benefits of the web as a useful source of hyperlinked
information comes from the careful choices made in crafting the
recommendations that link a page to closely related pages. Though this
advantage was identified well before the WWW was in place by
Bush~\cite{Bush45aswe}, it continues to persist even today. The
presence of recommendations is an integral feature of several popular
websites that are known to have high user engagement and long
sessions. Examples range from entertainment sites like YouTube that
recommend a set of videos on the right for every video watched by a
user, information sites like Quora that recommend a set of related
questions on every question page, to retail sites like Amazon that
recommend similar products on every product page. Recent
estimates~\cite{big-data-book13} attribute up to a third of the sales
on Amazon and three-quarters of new orders on Netflix are influenced by precisely choosing recommendations.

Recommendation systems~\cite{Schafer1999, Adomavicius2005,
  Resnick1997} start by finding a large set of related candidate items
for each item (or page) using relevance. In this work, we assume $d$
such related candidates are available per page and our goal is to
analyze how to prune the set to $c < d$ recommendations such that
globally we ensure that the resulting recommendation subgraph can be traversed
`efficiently' by the user.

\subsection{Our Contributions}
While optimal solutions to some versions of the recommendation subgraph problem can be obtained by using a maximum matching algorithm, such algorithms are too costly to run on real-life instances. We introduce three simple alternate methods that can be implemented in linear or near-linear time and examine their properties. We show how these simpler methods perform very well on simulated data, and have very effective running times.

Two of our simplest methods have been deployed at BloomReach and we show the performance of these methods on data from several retailers that are clients of BloomReach with varying sizes of their sites. Our empirical results clearly show that only the two simplest methods that we study are practically scalable for real-world problems.

To summarize, our contributions are the development of a new structural model for recommendation systems as a subgraph selection problem for maximizing discoverability, the proposal of very simple methods to solve them at scale along with some associated theoretical performance guarantee analysis, and an empirical validation of our conclusions with simulated and real-life data.

