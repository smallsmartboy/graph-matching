\section{Introduction}

\subsection{Web Relevance Engines}
The digital discovery divide~\cite{WebRelevanceEngine} refers the problem of companies not being able to present users with what they seek in the short time they spend looking for this information.
The problem is prevalent not only in e-commerce websites but also in social networks and micro-blogging sites where surfacing relevant content quickly is important for user engagement. \vs

BloomReach is a big-data marketing company that uses the client's content as well as web-wide data to optimize both customer acquisition and satisfaction for e-retailers.
BloomReach's clients include popular retailers like Nieman Marcus, Crate \& Barrel, Williams-Sonoma and Staples besides many others. In this paper, we describe the structure optimizer component of BloomReach's Web Relevance Engine.
This component works on top of the recommendation engine so as to carefully add a set of links across pages that ensures that crawlers as well as users can efficiently navigate the entire website.

\subsection{Structure Optimization of Websites}

One of the great benefits of the web as a useful source of hyperlinked
information comes from the careful choices made in crafting the
recommendations that link a page to closely related pages. Though this
advantage was identified well before the WWW was in place by
Bush~\cite{Bush45aswe}, it continues to persist even today.
Recent estimates~\cite{big-data-book13} attribute up to a third of the sales
on Amazon and three-quarters of new orders on Netflix to users that are
influenced by the carefully chosen recommendations provided to them. \vs

Even though recommendations exist across the entire www, we provide some simple concrete examples. First, YouTube has a section that displays all the related videos for every main video being viewed.
Quora has a section for questions related to the main question that is displayed. These recommendations are critical in determining how the traffic across all of YouTube or Quora is going to flow.
An important concern of website owners is whether a significant fraction of the site is not recommended at all (or `hardly' recommended) from other more popular pages. Continuing
with the above example, if a large fraction of the YouTube videos were not recommended from any (or few) other videos, then millions of great videos will lie undiscovered. One way to address this
problem is to try to ensure that every page will obtain at least a baseline number of visits so that great content does not remain undiscovered, and thus bridge the discovery divide mentioned above. \vs

We use the criterion of discoverability as the objective for the choice of the links to recommend. Consequently, we get a new formulation of the recommendation selection problem that is structural. In particular, we think of commonly visited pages in a site as the already discovered pages, from which there are a large number of possible recommendations available to related but less visited peripheral pages. The problem of choosing a limited number of pages to recommend at each discovered page can be cast with the objective of maximizing the number of peripheral non-visited pages that are linked. We formulate this as a recommendation subgraph problem, and study practical algorithms for solving these problems in real-life data. \vs

\subsection{Recommendation Systems as a Subgraph Selection Problem}

Formally, we divide all pages in a site into two groups: the discovered pages and the undiscovered ones.
Furthermore, we assume that recommendation systems~\cite{Schafer1999, Adomavicius2005,
  Resnick1997} find a large set of related candidate undiscovered page recommendations
for each discovered page using relevance. In this work, we assume $d$
such related candidates are available per page creating a candidate recommendation bipartite graph 
(with degree $d$ at each discovered page node).
Our goal is to analyze how to prune this set to $c < d$ recommendations such that
globally we ensure that the resulting recommendation subgraph can be navigated efficiently by the user to enable better discovery. \vs

\subsection{Our Contributions}
While optimal solutions to some versions of the recommendation subgraph problem can be obtained by using a maximum matching algorithm, such algorithms are too costly to run on real-life instances. We introduce three simple alternate methods that can be implemented in linear or near-linear time and examine their properties. 
In particular, we delineate when
each method will work effectively on popular random graph models, and when a practitioner needs will need to employ a more sophisticated algorithm. 
We then evaluate how these simple methods perform on simulated data, both in terms of solution quality and  running time.
Finally, we show the deployment of these methods on BloomReach's real-world client link graph and measure their
actual performance in terms of running-times, memory usage and accuracy. \vs

To summarize, our contributions are as follows.
\begin{enumerate}
\item The development of a new structural model for recommendation systems as a subgraph selection problem for maximizing discoverability, 
\item The proposal of three methods with increasing sophistication to solve the problem at scale along with associated theoretical performance guarantee analysis for each method, and
\item An empirical validation of our conclusions with simulated and real-life data.
\end{enumerate}
