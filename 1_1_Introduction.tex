\section{Introduction}

\subsection{Web Relevance Engines}
Digital discovery divide (cite Bloomreach white paper) is the problem of companies not being able to present users with what they seek in the short time they spend looking for this information.

The problem is prevalent not only in eCommerce firms but also in social networks and micro-blogging sites where surfacing relevant content quickly is important for user engagement.

BloomReach is a big-data marketing company that uses the client's content as well as web-wide data to optimize both customer acquisition and satisfaction.
BloomReach's clients include popular retailers like Neiman Marcus, Crate \& Barrel, Williams-Sonoma and Staples. In this paper, we describe the structure optimizer of the Web Relevance Engine.
The structure optimizer adds hyperlinks between pages that ensures that crawlers as well as users can efficiently navigate the entire website of a client. 

\subsection{Structure Optimization of Websites}
Similar problems arise in all other recommendation systems where the set of possible relevant recommendations from each page is typically much larger than can be displayed as links in the page. 
To just provide a couple of simple examples, YouTube has a section that displays all the related videos for the main video being viewed. Quora has a section for questions related to the main question
that is displayed. These recommendations are central to determining how the traffic across say all of YouTube is going to flow.

As a website owner one big question that you can ask is simply if there is a significant fraction of the website that is not recommended at all (or `hardly' recommended) from other pages. Continuing
with the above example, if say half of the YouTube videos hardly were recommended from other videos, then millions of great videos might be undiscovered for the longest time. We try to address
problem by ensuring that every page will require at least a baseline number of visits so that great content does not remain undiscovered. 

We use the criterion of discoverability as the objective for the choice of the links to recommend. We naturally get a new formulation of the recommendation selection problem that is structural. In particular, if we think of commonly visited pages in a site as the discovered pages, from which there are a large number of possible recommendations available to related but less visited peripheral pages, the problem of choosing a limited number of pages to recommend at each discovered page can be cast with the objective of maximizing the number of peripheral non-visited pages that are linked. We formulate this as a recommendation subgraph problem, and study practical algorithms for solving these problems in real-life data. 

\subsection{Recommendation Systems as a Subgraph Selection Problem}

(From previous writeup - needs to be integrated better)
One of the great benefits of the web as a useful source of hyperlinked
information comes from the careful choices made in crafting the
recommendations that link a page to closely related pages. Though this
advantage was identified well before the WWW was in place by
Bush~\cite{Bush45aswe}, it continues to persist even today. The
presence of recommendations is an integral feature of several popular
websites that are known to have high user engagement and long
sessions. Examples range from entertainment sites like YouTube that
recommend a set of videos on the right for every video watched by a
user, information sites like Quora that recommend a set of related
questions on every question page, to retail sites like Amazon that
recommend similar products on every product page. Recent
estimates~\cite{big-data-book13} attribute up to a third of the sales
on Amazon and three-quarters of new orders on Netflix are influenced by precisely choosing recommendations.

Recommendation systems~\cite{Schafer1999, Adomavicius2005,
  Resnick1997} start by finding a large set of related candidate items
for each item (or page) using relevance. In this work, we assume $d$
such related candidates are available per page and our goal is to
analyze how to prune the set to $c < d$ recommendations such that
globally we ensure that the resulting recommendation subgraph can be traversed
`efficiently' by the user.

\subsection{Our Contributions}
While optimal solutions to some versions of the recommendation subgraph problem can be obtained by using a maximum matching algorithm, such algorithms are too costly to run on real-life instances. We introduce three simple alternate methods that can be implemented in linear or near-linear time and examine their properties. We show how these simpler methods perform very well on simulated data, and have very effective running times.

Two of our simplest methods have been deployed at BloomReach and we show the performance of these methods on data from several retailers that are clients of BloomReach with varying sizes of their sites. Our empirical results clearly show that only the two simplest methods that we study are practically scalable for real-world problems.

To summarize, our contributions are the development of a new structural model for recommendation systems as a subgraph selection problem for maximizing discoverability, the proposal of very simple methods to solve them at scale along with some associated theoretical performance guarantee analysis, and an empirical validation of our conclusions with simulated and real-life data.

