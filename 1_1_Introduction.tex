\section{Introduction}

\subsection{Web Relevance Engines}
The digital discovery divide~\cite{WebRelevanceEngine} refers to the problem of companies not being able to present users with what they seek in the short time they spend looking for this information. The problem is prevalent not only in e-commerce websites but also in social networks and micro-blogging sites where surfacing relevant content quickly is important for user engagement. \vs

BloomReach is a big-data marketing company that uses the client's content as well as web-wide data to optimize both customer acquisition and satisfaction for e-retailers.
BloomReach's clients include popular retailers like Nieman Marcus, Crate \& Barrel, Williams-Sonoma and Staples besides many others. In this paper, we describe the structure optimizer component of BloomReach's Web Relevance Engine. This component works on top of the recommendation engine so as to carefully add a set of links across pages that ensures that users can efficiently navigate the entire website.

\subsection{Structure Optimization of Websites}

%One of the great benefits of the web as a useful source of hyperlinked
%information comes from the careful choices made in crafting the
%recommendations that link a page to closely related pages. Though this
%advantage was identified well before the WWW was in place by
%Bush~\cite{Bush45aswe}, it continues to persist even today.
%Recent estimates~\cite{big-data-book13} attribute up to a third of the %sales
%on Amazon and three-quarters of new orders on Netflix to users that are
%influenced by the carefully chosen recommendations provided to them. \vs

%Even though recommendations exist across the entire web, we provide some simple concrete examples. First, YouTube has a section that displays all the related videos for every main video being viewed.
%Quora has a section for questions related to the main question that is displayed. These recommendations are critical in determining how the traffic across all of YouTube or Quora is going to flow.

An important concern of retail website owners is whether a significant fraction of the site is not recommended at all (or `hardly' recommended) from other more popular pages within their site. Moreover, crawlers building search engine indices typically require redundant coverage from several popular pages before they include such less popular pages in their results.
%Continuing
%with the above example, if a large fraction of the YouTube videos were not recommended from any (or few) other videos, then millions of great videos will lie undiscovered. 
One way to address this
problem is to try to ensure that every page will obtain at least a baseline number of links from popular pages so that great content does not remain undiscovered, and thus bridge the discovery divide mentioned above. \vs
%This baseline number of links is also an important parameter used by search engine crawlers to include in their indexes of these retail sites.\vs

We use this criterion of discoverability as the objective for the choice of the links to recommend. 
We start with a small set of already discovered or popular nodes available at a site, and want to use this set to make as many new nodes discoverable as possible. This objective leads to a new structural formulation of the recommendation selection problem. In particular, we think of commonly visited pages in a site as the already discovered pages, from which there are a large number of possible recommendations available (using more traditional information retrieval methods) to related but less visited peripheral pages. The problem of choosing a limited number of pages to recommend at each discovered page can be cast with the objective of maximizing the number of peripheral non-visited pages that are redundantly linked. We formulate this as a recommendation subgraph problem, and study practical algorithms for solving these problems at scale with real-life data.\vs
%We start with a small set of already discovered nodes, and want to grow this set of nodes as quickly as possible. This process may then be iterated as necessary to facilitate the discovery of more and more items. This new objective leads to a new formulation of the recommendation selection problem that is structural. In particular, we think of commonly visited pages in a site as the already discovered pages, from which there are a large number of possible recommendations available to related but less visited peripheral pages. The problem of choosing a limited number of pages to recommend at each discovered page can be cast with the objective of maximizing the number of peripheral non-visited pages that are linked. We formulate this as a recommendation subgraph problem, and study practical algorithms for solving these problems in real-life data. \vs

\subsection{Recommendation Systems as a Subgraph Selection Problem}

Formally, we divide all pages in a site into two groups: the discovered pages and the undiscovered ones.
Furthermore, we assume that traditional recommendation systems~\cite{Schafer1999, Adomavicius2005,
  Resnick1997} provide us with a large set of related candidate undiscovered page recommendations
for each discovered page using relevance metrics. In this work, we assume $d$
such related candidates are available per page creating a candidate recommendation bipartite graph
(with degree $d$ at each discovered page node).
Our goal is to analyze how to prune this set to $c < d$ recommendations such that
globally we ensure that the number of undiscovered pages that have at least $a \geq 1$ recommendations to them in the chosen subgraph. This gives the $(c,a)$-recommendation subgraph introduced in Section~\ref{recsub}. Even though the case of $a=1$ reduces to a polynomially solvable version of a matching problem, the more usual cases of $a > 1$ are most likely NP-hard prohibiting exact solution methods at scale. Even the simple versions that reduce to matching are too computational expensive on memory and processing to run on real-life instances \vs
%the resulting recommendation subgraph can be navigated efficiently by the user to enable better discovery. \vs

\subsection{Our Contributions}

%While optimal solutions to some versions of the recommendation subgraph problem can be obtained by using a maximum matching algorithm, such algorithms are too costly to run on real-life instances. 
We introduce three simple heuristic methods that can be implemented in linear or near-linear time and thoroughly investigate their theoretical performance.
In particular, we delineate when each method will work effectively on popular random graph models, and when a practitioner will need to employ a more sophisticated algorithm. We then evaluate how these simple methods perform on simulated data, both in terms of solution quality and  running time.
Finally, we show the deployment of these methods on BloomReach's real-world client link graph and measure their actual performance in terms of running-times, memory usage and accuracy. It is worthwhile to note that the simplest of the three methods that we propose (sampling) can be easily adapted to the incremental dynamic setting when the set of pages and candidate recommendations is changing rapidly.
\vs

To summarize, our contributions are as follows.
\begin{enumerate}
\item The development of a new structural model for recommendation systems as a subgraph selection problem for maximizing discoverability (Section~\ref{modelsec}).
\item The proposal of three methods (sampling, greedy and partition) with increasing sophistication to solve the problem at scale along with associated theoretical performance guarantee analyses (Section~\ref{methodsec}). In particular, we show very strong theoretical bounds on the size of the discoverable set for the sampling algorithm in the fixed degree random graph model (Theorem~\ref{original_result}); in the Erd\"os-Renyi model for the greedy algorithm (Theorem~\ref{greed-is-good}) and for a partition-based algorithm (Theorem~\ref{perfect}).
\item An empirical validation of our conclusions with simulated and real-life data (Section~\ref{expsec}). Our simulations show that sampling is the least resource intensive and performs satisfactorily, while partition is the most resource intensive but performs better for small values of discoverability threshold $a$; Greedy is the overall best-performer using a single pass over the data and producing good results over a variety of parameters. In the tests with real retailer data, we see these trends broadly reflected in the results: Greedy performs well when $c$ gets moderately large giving almost optimal starting from $a=2$. The partition method is promising when the targeted $a$ value is low. Sampling is typically worse than greedy, but unlike the partition algorithm, its performance improves dramatically as $c$ becomes larger, and does not worsen as quickly when $a$ gets larger.
\end{enumerate}
