\section{Algorithms for Recommendation Subgraphs}
In this section, we analyze the lazy algorithm of choosing any set of $c$ recommendations, and the slightly more interesting greedy algorithm for finding a $(c,a)$-recommendation subgraph. To do this, we first introduce the fixed-degree random graph model for the candidate supergraph of recommendations. 

\subsection{Fixed Degree Model}
\label{fixed-degree}

In this model, we assume that a bipartite graph $G=(L,R,E)$ is
generated probabilistically by the following procedure. Each
vertex $v\in L$, uniformly samples a set of $d$ neighbors
from $R$. For convenience let $|L|=l$, $|R|=r$ and $k=l/r$. From
$G$, we sample a subgraph $H$ where for each vertex $u\in L$ a set of
$c$ neighbors are sampled uniformly from set of incident vertices. The
following theorem derives a lower bound on the expected size $S$ of the
number $v\in R$ such that $\deg_H(v) \geq 1$.

\begin{thm}\label{original_result}
Suppose that $G=(L,R,E)$ and $H\subseteq G$ is generated as above. Then
\[ \E[S] \geq r(1-\exp(-ck))\]
where the expectation is over the random sampling of $G$ and $H$.
\end{thm}
\begin{proof}
For each $v\in R$ let $X_v$ be the indicator variable for the event
that $\deg_H(v) \geq 1$. Note that since for each vertex $u\in L$, $H$
uniformly samples from a uniformly sampled set of neighbors, we can
think $H$ as being generated by the same process that generated $G$,
but with $d$ replaced with $c$. Now for a specific vertex $u \in R$,
the probability that it has no incident edges is
$\left(1-\frac{1}{r}\right)^c$. Since the selection of neighbors for each
vertex in $L$ is independent, it follows that that:
\[ \Pr[X_v=0] = \left(1-\frac{1}{r}\right)^{cl} \leq \exp\left(-c \cdot \frac{l}{r}\right) = \exp(-ck) \]
Note that $S = \sum_{v\in R} X_v$. Applying linearity of expectation, we get
\[ \E[S] = \sum_{v\in V} \E[X_v] \geq r(1-\exp(-ck))\]
\end{proof}

While this shows a lower bound in absolute terms, we must compare it to the best possible solution {\em OPT}. The follow theorem proves the approximation ratio to {\em OPT}.

\begin{thm}
The above sampling algorithm gives a $1-1/e$ factor approximation to the $(c,1)$-graph recommendation problem in expectation.
\end{thm}
\begin{proof}
The size of the optimal solution is bounded above by both the number
of edges in the graph and the number of vertices in $R$. The former of
these is $cl=ckr$ and the latter is $r$, which shows that $OPT \leq
r\max(ck,1)$. Therefore, by simple case analysis the approximation ratio in
expectation is at least
\[ \frac{1-\exp(-ck)}{\min(ck,1)} \geq 1-\frac{1}{e} \]
\end{proof}

However in reality, the approximation obtained by this sampling
approach can be much better for certain values of $ck$. In particular,
if $ck>1$, then the approximation ratio is $1-\exp(-ck)$, which
approaches 1 as $ck\to\infty$. In particular, if $ck=3$, then the
solution will be at least 95\% as good as the optimal solution even
with our trivial bounds. Similarly, when $ck<1$, the approximation
ratio is $(1-\exp(-ck))/ck$ which also approaches 1 as $ck\to 0$. In
particular, if $ck=0.1$ then the solution will be at 95\% as good as
the optimal solution. The case when $ck=1$ therefore represents the
worst case outcome for this model where we only guarantee 63\%
optimality. The graph below shows the approximation ratio as a
function of $ck$.\vs

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{Sri_Original.png}
\caption{Approximation Ratio as a function of $ck$ }
\end{figure}

Now suppose that $G$ is generated and $H$ is sampled using the same
processes as described above. In the next theorem, we extend the above
bounds to the $(c,a)$-graph recommendation problem where $a>1$.
%In particular if we set $a=1$, we will obtained the estimate from the original analysis.

\begin{thm}
Let $S$ be the random variable denoting the number of vertices $v \in R$ such that $\deg_{H}(v)\geq a$. Then
\[ \emph{\E}[S] \geq r\left(1-e^{-ck+\frac{a-1}{r}}\frac{(ck)^a-1}{ck-1}\right)  \]
where the expectation is over the randomness of $G$ and $H$.
\end{thm}

\begin{proof}
Let $X_{uv}$ be the indicator variable of the event that the edge $uv$
(note that $u\in L$ and $v\in R$) is in the subgraph that we picked
and set $X_{v} = \sum_{u\in U} X_{uv}$ so that $X_{v}$ represents the
random degree of the vertex $v$ in our subgraph. Because our algorithm
uniformly subsamples a uniformly random selection of edges, we can
assume that $H$ was generated the same way as $G$ but sampled $c$
instead of $d$ edges for each vertex $u\in L$. So $X_{uv}$ is a
bernoulli random variable. Using the trivial bound $\binom{n}{i}
\leq n^i$ on binomial coefficients we get:
\begin{align*}
      \Pr[X_v < a]
&=    \sum_{i=0}^{a-1} \binom{cl}{i} \left(1-\frac{1}{m}\right)^{cl-i}\left(\frac{1}{r}\right)^i \\
&\leq \sum_{i=0}^{a-1} \left(\frac{cl}{r}\right)^i\left(1-\frac{1}{r}\right)^{cl-i} \\
&\leq    \left(1-\frac{1}{r}\right)^{cl-(a-1)}\sum_{i=0}^{a-1} (ck)^i \\
&\leq \left(1-\frac{1}{r}\right)^{cl-(a-1)}\frac{(ck)^a-1}{ck-1} \\
&\leq e^{-ck+\frac{a-1}{r}} \frac{(ck)^a-1}{ck-1}
\end{align*}


Letting $Y_v = \left[X_v \geq a\right]$, we now see that

\[ \E[S] = \E\left[\sum_{v\in R} Y_v\right] \geq r\left(1-e^{-ck+\frac{a-1}{r}} \frac{(ck)^a-1}{ck-1}\right) \]
\end{proof}

Now, we can perform a similar analysis as before. In
particular, if $ck>a$, then the problem is easy on average though we
need $ck$ to get larger than before to be close to optimal. This
is in comparison to the trivial estimate of $cl$. For a fixed $a$, a
random solution gets better as $ck$ increases because the decrease in
$e^{-ck}$ more than compensates for the polynomial in $ck$ next to
it. However, in the more realistic case $ck<a$, we need to use the trivial estimate of $ckr/a$, and the analysis from the previous section does not extend
here. \\



In both this analysis and the previous one, $ck$ is the average degree
of a vertex $v\in R$ in our chosen subgraph. The original analysis showed that if $ck>1$, then the sampling algorithm will probably cover every vertex in $R$ since the expected degree of each vertex is large. On the other hand if $ck$ is small ($ck < 1$) then the best possible solution is obtained when none of the vertices
in $R$ has degree greater than 1. 

If $ck<1$, then we do not cover very
many vertices in $R$, but we also do not cover many vertices more than
once. Since the optimal solution in this case was correspondingly low,
our solution was good in the $a=1$ case. However, when $a<1$, the fact
that our edges are well-dispersed only hurts our solution because we
need to concentrate the edges on particular nodes in $R$ that will eventually 
count in the objective. The following table shows how large $ck$ needs to be for the
solution to be 95\% optimal for different values of $a$.
\begin{figure}[h]
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
    $a$ & 1 & 2 & 3 & 4 & 5 \\ \hline
    $ck$ & 3.00 & 4.74 & 7.05 & 10.01 & 13.48 \\
    \hline
  \end{tabular}
  \caption{The required $ck$ to obtain 95\% optimality}
\end{figure} 

We close out this section by showing that the main result that holds in expectation also hold with high probability using concentration bounds. 
%We can convert the expectation results to a lower bound that holds with high probability using concentration results like Chernoff bounds. 
While Chernoff bounds are usually stated for independent variables, the
variant below holds for any number of pairwise non-positively correlated
variables.

\begin{thm}\label{negative_corr_chernoff}~\cite{AugerDoerr2011}
Let $X_1,\ldots, X_n$ be non-positively correlated variables. If $X=\sum_{i=1}^n X_i$, then for any $\delta\geq 0$
\[ Pr[X \geq (1+\delta)\E[X] ] \leq \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{\E[X]} \]
\end{thm}

Using this result, we can convert our expectation result to one that holds
with high probability as follows:

\begin{thm}
Let $S$ be the random variable denoting the number of vertices $v \in R$ such that $\deg_{H}(v)\geq a$. Then
$ S \leq r(1-2\exp(-ck))$ with probability at most $(e/4)^{r(1-\exp(-ck))}$.
\end{thm}

\begin{proof}
We can write $S$ as $\sum_{v\in R} 1-X_v$ where $X_v$ is the indicator
variable that denotes that $X_v$ is matched. Note that the variables 
$1-X_v$ for each $v\in R$ are non-positively correlated. In particular,
if $N(v)$ and $N(v')$ are disjoint, then $1-X_v$ and $1-X_{v'}$ are 
independent. Otherwise, $v$ not claiming any edges can only increase 
the probability that $v'$ gets an edge from any vertex
$u\in N(v)\cap N(v')$, so the variables would be negatively correlated.
Furthermore, note that the expected size of $S$ is $r(1-\exp(-ck))$ by
Theorem \ref{original_result}. Therefore, we can apply Theorem 
\ref{negative_corr_chernoff} with $\delta=1$ and obtain the result.
\end{proof}
