\abstract

Recommendations are central to the utility of many of the popular websites such as YouTube,
Facebook and Quora, as well as the economic viability of e-commerce websites.
Such sites typically contain a set of recommendations on every product
page which enables visitors to easily navigate the website. Choosing
an appropriate set of recommendations at each page is one of the key
features of web relevance engines that have been commercially deployed 
at several retail sites.

Specifically in the case of BloomReach, an engine consisting of several independent
software components analyzes and optimizes its clients websites.
%Can cite ~\cite{WebRelevanceEngine} if needed.
This paper focuses on the structure optimizer component which
improves the website navigation experience so as to enable the discovery of previously
undiscovered content.

We begin the paper by formalizing the concept of recommendations that can be used to improve
the structure and discoverability of the website. We formulate this as a natural graph optimization
problem which, in its simplest case, reduces to a bipartite matching problem. In practice, solving these
matching problems requires super-linear time and is not scalable. Furthermore, implementing simple 
algorithms is paramount in practice because they are
significantly easier to maintain in a production software package. This motivated us to analyze 
three heuristic methods for solving the problem in increasing order of sophistication: 
a local random sampling algorithm,
a greedy algorithm and a partitioning algorithm that divides the edges into sets from which recommendations
are separately chosen.

We first theoretically analyze the performance of these three methods
on random graph models characterizing when each method will yield a solution of sufficient quality
and the parameter regimes when more sophistication is needed. 
We complement this by providing an empirical analysis
of these algorithms on simulated and real-world production data. Our
results confirm that it is not always necessary to implement complicated
algorithms in the real-world. Indeed, our results demonstrate that 
very good practical results 
can be obtained by using simple heuristics and 
backed by the confidence of concrete theoretical guarantees.

\iffalse

We formulate the problem of choosing the short list of recommendations from each page as one of choosing a subgraph of the candidate recommendation graph with the objective of maximizing the size of the recommended pages.

To inter-connect the website for efficient
traversal it is critical to choose a few recommendations on each
page from the list of all candidates.

In our formulation of the problem, the set of pages on the site is
divided into a few popular pages where surfers arrive and the
remaining large number of pages that should be recommended from the
popular pages. The natural bipartite graph of potential
recommendations is a candidate supergraph. Given the limited space to
display recommendations a subgraph that has bounded outdegree $c$ on the
popular pages must be chosen. Our goal is to maximize the number of
undiscovered pages that have indegree at least some target $a$. We introduce and study this
as the $(c, a)$-recommendation subgraph problem.

Solving such problems optimally at web-scale typically involves
writing distributed matching algorithms.  These can be incredibly hard
to implement, debug and scale effectively.  Instead, in this work, we
study the effectiveness of a lazy engineer solving the recommendation
subgraph problem. We investigate the cases when the candidate
supergraph is a random graph under two models: the fixed-degree model
where every node on the left has exactly $d$ random neighbors on the
right, as well as the standard Erd\"{o}s-Renyi model with expected
degree $d$ on the left side. We show that for most reasonable
parameters of the models, the lazy engineer would have found solutions
very close to optimal. We further show the conditions under which a
perfect recommendation subgraph, a generalization of perfect
matchings, exists. Lastly, to more realistically model web graphs, we
propose generalizations of the random graph models using topic
taxonomies, varying subgraph edge densities and edge weights that
capture the strength of recommendations. Surprisingly, the lazy
engineer would still have found near optimal solutions under different
realistic parameters, which we validate with some computational
testing on simulated models.
\fi 