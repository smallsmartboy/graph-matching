\section{Algorithms for Recommendation Subgraphs}

\subsection{The Sampling Algorithm}
\label{fixed-degree}

We present the sampling algorithm for the $(c,a)$-recommendation subgraph formally below.

\begin{algorithm}[h]
  \SetAlgoLined
  \KwData{A bipartite graph $G=(L,R,E)$}
  \KwResult{A $(c,a)$-recommendation subgraph $H$}
  \For{$u$ in $L$}{
    $S \leftarrow$ a random sample of $c$ vertices without replacement in $N(u)$\;
    \For{$v$ in S} {
      $H \leftarrow H \cup \{(u,v)\}$\;
   	}
  }
  \Return $H$\;
  \caption{The sampling algorithm}
  \label{sampling-algo}
\end{algorithm} \vs

Given a bipartite graph $G$, the algorithm has runtime complexity of
$O(|E|)$ since every edge is considered at most once. The space
complexity can be taken to be $O(1)$, since the adjacency representation
of $G$ can be assumed to be pre-sorted by the endpoint of each edge in $L$.\vs

We next introduce a simple random graph model for the supergraph from
which we are allowed to choose recommendations and present a bound on
its expected performance when the underlying supergraph $G=(L,R,E)$ is
chosen probabilistically according to this model.
\vs

{\bf Fixed Degree Model:} In this model for generating the candidate
recommendation graph, each vertex $u\in L$ uniformly and independently
samples $d$ neighbors from $R$ with replacement. While this allows each
vertex in $L$ to have the same vertex as a neighbor multiple times, in
reality $r$ is so much larger than $d$ that repetition is very unlikely
at a vertex in $L$. This model is similar to, but is distinct from the more
commonly known Erd\"{o}s-Renyi model of random graphs~\cite{janson2011random}.
In particular, while the degree of each vertex in $L$ is fixed under
our model, concentration bounds can show that the degrees of the
vertices in $L$ would have similarly been concentrated around $d$ for $p=d/r$
in the Erd\"{o}s-Renyi model. We prove 
the following theorem about the performance of the Sampling Algorithm.
We denote the ratio of the size of $L$ and $R$ by $k$, i.e., we define
$k = \frac{l}{r}$.

\begin{thm}\label{original_result}
Let $S$ be the
random variable denoting the number of vertices $v \in R$ such that
$\deg_{H}(v)\geq a$ in the fixed-degree model. Then
\[ \emph{\E}[S] \geq r\left(1-e^{-ck+\frac{a-1}{r}}\frac{(ck)^a-1}{ck-1}\right)  \]
\end{thm}

\begin{proof}
We will analyze the sampling algorithm as if it picks the neighbors of
each $u\in L$ with replacement, the same way the fixed-degree model
generates $G$. This variant would obviously waste some edges, and perform
worse than the variant which samples neighbors without replacement. This
means that any performance guarantee we prove for this variant holds
for our original statement of the algorithm as well. \vs

To prove the claim let $X_{v}$ be the random variable that represents
the degree of the vertex $v\in R$ in our chosen subgraph $H$. Because our
algorithm uniformly subsamples a uniformly random selection of edges,
we can assume that $H$ was generated the same way as $G$ but sampled $c$
instead of $d$ edges for each vertex $u\in L$. Since there are $cl$
edges in $H$ that can be incident on $v$, and each of these edges has a 
$1/r$ probability of being incident on a given vertex in $L$, we can now
calculate that

\begin{align*}
      \Pr[X_v = i]
&=    \binom{cl}{i} (1-\frac{1}{r})^{cl-i} \left(\frac{1}{r}\right)^i \\
&\leq (cl)^i (1-\frac{1}{r})^{cl-i} \left(\frac{1}{r}\right)^i
\end{align*}

Using a union bound, we can combine these inequalities to upper bound
the probability that $\deg_H(v)<a$.

\begin{align*}
      \Pr[X_v < a]
&=    \sum_{i=0}^{a-1} \binom{cl}{i} \left(1-\frac{1}{r}\right)^{cl-i}\left(\frac{1}{r}\right)^i \\
&\leq \sum_{i=0}^{a-1} \left(\frac{cl}{r}\right)^i\left(1-\frac{1}{r}\right)^{cl-i} \\
&\leq    \left(1-\frac{1}{r}\right)^{cl-(a-1)} \sum_{i=0}^{a-1} (ck)^i \\
&\leq \left(1-\frac{1}{r}\right)^{cl-(a-1)}\frac{(ck)^a-1}{ck-1} \\
&\leq e^{-ck+\frac{a-1}{r}} \frac{(ck)^a-1}{ck-1}
\end{align*}

Letting $Y_v = \left[X_v \geq a\right]$, we now see that

\[ \E[S] = \E\left[\sum_{v\in R} Y_v\right] \geq r\left(1-e^{-ck+\frac{a-1}{r}} \frac{(ck)^a-1}{ck-1}\right) \]
\end{proof} \vspace{-.2cm}

We can combine this lower bound with a trivial upper bound to obtain an
approximation ratio that holds in expectation.  

\begin{thm}
The above sampling algorithm gives a $\left(1-\frac1e\right)$-factor approximation to the $(c,1)$-graph recommendation problem in expectation.
\end{thm}
\begin{proof}
The size of the optimal solution is bounded above by both the number
of edges in the graph and the number of vertices in $R$. The former of
these is $cl=ckr$ and the latter is $r$, which shows that the optimal solution size
$OPT \leq
r\max(ck,1)$. Therefore, by simple case analysis the approximation ratio
in expectation is at least $({1-\exp(-ck)})/\min(ck,1) \geq 1-\frac{1}{e} $
\end{proof}


For the $(c, 1)$-recommendation subgraph problem the approximation obtained by this sampling approach can be much better for certain values of $ck$. In particular,
if $ck>1$, then the approximation ratio is $1-\exp(-ck)$, which
approaches 1 as $ck\to\infty$. In particular, if $ck=3$, then the
solution will be at least 95\% as good as the optimal solution even
with our trivial bounds. Similarly, when $ck<1$, the approximation
ratio is $(1-\exp(-ck))/ck$ which also approaches 1 as $ck\to 0$. In
particular, if $ck=0.1$ then the solution will be at 95\% as good as
the optimal solution. The case when $ck=1$ represents the
worst case outcome for this model where we only guarantee 63\%
optimality. Figure~\ref{fig:simple_approx} shows the approximation ratio as a
function of $ck$ for the $(c,1)$-recommendation subgraph problem in the fixed degree model.\vs

\begin{figure}[H]
  \centering
  \includegraphics[width=0.34\textwidth]{images/sri_Original.png}
  \caption{Approx ratio as a function of $ck$ }\label{fig:simple_approx}
\end{figure}

For the general $(c, a)$-recommendation subgraph problem, if $ck>a$,
then the problem is easy on average. This is in comparison to the
trivial estimate of $cl$. For a fixed $a$, a random solution gets
better as $ck$ increases because the decrease in $e^{-ck}$ more than
compensates for the polynomial in $ck$ next to it. However, in the
more realistic case, the undiscovered pages in $R$ too numerous to be all covered even if we used the full set of budgeted links allowed out of $L$, i.e. $cl < ra$ or rearranging, $ck<a$; in this case, we need to use the trivial estimate of
$ckr/a$, and the analysis for $a=1$ does not extend here. For practical purposes, the table
in Figure~\ref{a-values} shows how large $c$ needs to be (in terms of $k$) for the
solution to be 95\% optimal for different values of $a$, again in the fixed degree model.\vs

\begin{figure}[H]
  %\vspace{.2cm}
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
    $a$ & 1 & 2 & 3 & 4 & 5 \\ \hline
    $c$ & $3.00k^{-1}$ & $4.74k^{-1}$ & $7.05k^{-1}$ & $10.01k^{-1}$ & $13.48k^{-1}$ \\
    \hline
  \end{tabular}
  \caption{The required $ck$ to obtain 95\% optimality for $(c, a)$-recommendation subgraph}
  \label{a-values}
\end{figure}


We close out this section by showing that the main result that holds
in expectation also hold with high probability for $a=1$, using the
following variant of Chernoff bounds.

\begin{thm}\label{negative_corr_chernoff}~\cite{AugerDoerr2011}
Let $X_1,\ldots, X_n$ be non-positively correlated variables. If $X=\sum_{i=1}^n X_i$, then for any $\delta\geq 0$
\[ \Pr[X \geq (1+\delta)\emph{\E}[X] ] \leq \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{\E[X]} \]
\end{thm}

%Using this we can convert our expectation result to one that holds
%with high probability.

\begin{thm}
Let $S$ be the random variable denoting the number of vertices $v \in R$ such that $\deg_{H}(v)\geq 1$. Then
$ S \leq r(1-2\exp(-ck))$ with probability at most $(e/4)^{r(1-\exp(-ck))}$.
\end{thm}

%\begin{proof}
%We can write $S$ as $\sum_{v\in R} (1-X_v)$ where $X_v$ is the indicator
%variable that denotes that $X_v$ is matched. Note that the variables
%$1-X_v$ for each $v\in R$ are non-positively correlated. In
%particular, if $N(v)$ and $N(v')$ are disjoint, then $1-X_v$ and
%$1-X_{v'}$ are independent. Otherwise, $v$ not claiming any edges can
%only increase the probability that $v'$ has an edge from any vertex
%$u\in N(v)\cap N(v')$. Also note that the expected size of $S$ is
%$r(1-\exp(-ck))$ by Theorem \ref{original_result}. Therefore, we can
%apply Theorem \ref{negative_corr_chernoff} with $\delta=1$ to obtain
%the result.
%\end{proof}

For realistic scenarios where $r$ is very large, the above theorem gives very tight bounds on the size of the solution, also explaining the effectiveness of the simple sampling algorithm in such instances.
