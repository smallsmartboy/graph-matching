
\section{Introduction}

One of the great benefits of the web as a useful source of hyperlinked
information comes from the careful choices made in crafting the
recommendations that link a page to closely related pages. Though this
advantage was identified well before the www was in place by
Bush~\cite{Bush45aswe}, it continues to persist even today. The
presence of recommendations is an integral feature of several popular
websites that are known to have high user engagement and long
sessions. Examples range from entertainment sites like YouTube that
recommend a set of videos on the right for every video watched by a
user, information sites like Quora that recommend a set of related
questions on every question page, to retail sites like Amazon that
recommend similar products on every product page. Recent
estimates~\cite{big-data-book13} attribute up to a third of the sales
in Amazon and three-quarters of new orders in Netflix are due to precisely choosing recommendations.

\subsection{Recommendation Subgraphs}
Recommendation systems~\cite{Schafer1999, Adomavicius2005,
  Resnick1997} start by finding a large set of related candidate items
for each item (or page) using relevance. In this work, we assume $d$
such related candidates are available per page and our goal is to
analyze how to prune the set to $c < d$ recommendations such that
globally we ensure that the resulting recommendation subgraph can be traversed
`efficiently' by the user. We represent recommendations
by using a digraph where the vertices are items and a directed
edge $(u, v)$ is a recommendation from $u$ to $v$. Note that, under
this model, if $c=1$ and we require the chosen recommendation subgraph
to be (strongly) connected, a feasible solution is a Hamilton cycle
which is NP-complete to find~\cite{CLRS2001}. 

We generalize the graph recommendation model by using a bipartite
digraph, where one partition $L$ represents the set of items for which
we are required to suggest recommendations and the other partition $R$
represents the set of items that can be potentially recommended. If
needed, the same item can be represented in both $L$ and $R$.  As
before, the input to the problem is the graph where each
$L$-vertex has $d$ recommendations. Given the space restrictions to
display recommendations, the output is a subgraph where each vertex in
$L$ has $c < d$ recommendations. The goal is to maximize the number of
vertices that have in-degree at least a target integer $a$. We
introduce and study this $(c, a)$-recommendation subgraph problem in
this work.  Note that if $a=c=1$ this is simply the maximum bipartite
matching problem~\cite{LovaszPlummer1986}. If $a=1$ and $c > 1$, we
obtain a $b$-matching problem, that can be converted to a bipartite
matching problem~\cite{Gabow1983}.\vs

\noindent
{\bf Motivation from Practice.} Over the past few years, the first and
third authors have implemented complex graph algorithms in
cutting-edge web-technology companies including Google, Facebook and
BloomReach. There are two key requirements in making such graph algorithms
practical. The first is that the method used must be very simple to
implement, debug and deploy. The second is that the method must scale
gracefully with larger sizes. \vs

Matching algorithms require linear memory and super-linear run-time
which does not scale well. For example, the Facebook graph has over a
billion vertices\cite{FacebookNodes} and hundreds of billions of edges. A typical
e-commerce website with 100M product pages and 100 recommendation candidates per
product would require easily over 160GB in main memory to run matching
algorithms; this can be reduced by using graph compression techniques but that adds more technical difficulties in development and maintenance. In practice offline problem instances are solved by using distributed computing such as map-reduce~\cite{DeanGhemawat2004}. However, efficient map-reduce algorithms for graph problems are notoriously difficult and complicated. \vs

% http://www.longtail.com/the_long_tail/faq/
% The long tail: Why the future of business is selling less of more
% Growth dynamics of the World-Wide Web. Nature - Long tail by page
% Analysis of WWW traffic in Cambodia and Ghana - Zipf by page
% Location and time do matter: A long tail study of website requests: http://public.csusm.edu/ckumar/research/Kumar-etal-online-location-time-DSS.pdf
% shows % Zipf with beta=-1.365
%%

We now describe typical web graph characteristics by discussing the sizes of $L$, $R$, $c$ and $a$ in practice. It is well known
%and can be verified by prior experiences of the authors 
that, in most websites,  a small number of `head' pages contribute to a
significant amount of the traffic while a long tail of the remaining
pages contribute to the rest~\cite{HubermanAdamic1999, DuDemmerBrewer2006}. As
demonstrated by a prior measurement~\cite{KumarNorrisSun2009}
it is not unreasonable to expect 50\% of site traffic to be
contributed by less than 1\% (a few thousands) of the web pages while a
large number of tail pages (a few hundred thousand) contribute
the other half. This implies that in practice $L$ can
be up to two orders of magnitude smaller than $R$.  By observing
recommendations of Quora, Amazon, YouTube and our own work, 
typical values for $c$ range from 3 to 20 recommendations per page. Values of $a$ are harder to nail down but it typically ranges from $1$ to $10$. \vs


\subsection{Main Results}
We initiate the study of structural engineering of effective
recommendation systems by formulating and solving them as subgraph
problems. We offer very good bounds on the performance of algorithms
that a lazy engineer would prefer (random choice, greedy) under a
natural new fixed-degree model; we extend the analysis of perfect
matchings in random graphs to show the range of values of the graph
density for which perfect recommendation subgraphs exist; motivated by
empirical recommendation supergraphs that we analyze, we propose three
new random graph models (hierarchical, cartesian product and weighted)
extending our fixed-degree model to capture these real-life scenarios,
and extend our analysis of the random algorithm to all these
models. We now provide a short overview of these results. 

%We hope our work will serve as a starting point for a more
%thorough investigation of recommendation subgraph problems.

{\bf The Lazy Engineer.} The implementation challenges mentioned above
motivated us to investigate simple algorithms that are practical and can 
be implemented at very large scale. First, we simply analyze choosing a random
$c$ out of $d$ edges and investigating its performance.  Surprisingly,
we found this method to be very effective for a wide range of
realistic scenarios. This led us to carry out a theoretical
investigation using a {\em fixed-degree} random graph model for the
candidate supergraph, where every node on $L$ has recommendations to a
random subset of size $d$ nodes in $R$.  This is similar to another
model usually termed `$d$-out', where vertices from both sides of the
bipartition generate $d$ edges~\cite{FriezePittel2004}. Our main
result (Theorem~\ref{original_result} in Section~\ref{fixed-degree})
identifies the range of parameters involving $c,a,|L|, |R|$ where the
lazy algorithm is very effective. We also show that this is a
$(1-\frac1e)$- approximation algorithm in expectation, and high
probability bounds as well.  We then analyzed a {\em greedy} algorithm
that scans the nodes on $R$, and checks
if there are $a$ neighbors from $L$ that have not exhausted their
bound of $c$ edges and if so, it uses them to add
this node to the solution set. We study this algorithm in
Section~\ref{greedy}, where a simple $\frac{1}{a+1}$-approximation
result is followed by a very tight bound for realistic values of the
edge density under the usual Erd\"os-Renyi
model\cite{ErdosRenyi59}. This analysis serves to portend the
impressive overall performance of greedy in our computational
results. \vs
%In the subsequent Section~\ref{worst-vs-avg}, we compare and plot the worst-case
%performance of Greedy against the average case expected performance of the random
%or lazy algorithm, as a basis for our later computational comparisons in a
%similar vein. \vs
%We extend the models later to more
%realistic
% graphs including a hierarchical, cartesian and weighted
%graph
% models. \vs

\noindent
{\bf Perfect Recommendation Subgraphs.} Next we turn to the question of the conditions under which the extensions of perfect matchings (corresponding to the case $a=1$) exist in random candidate supergraphs under the standard Erd\"os-Renyi model. Since at most $c$ edges are allowed out of each of $l$ nodes, and nodes on the right require degree $a$, the maximum number of nodes on the right that can have degree $a$ is at most $\frac{cl}{a}$. When a subgraph achieving this bound exists, we say that there is an perfect $(c,a)$-recommendation subgraph. We extend prior results on the
existence of a perfect matching in this model to characterize the edge probability values above which there exist such perfect recommendation subgraphs, by using a novel subset partitioning method for the analysis. \vs
 
%By relaxing the condition of optimality of matching (i.e.
%perfect matchings) to disallowing short augmenting paths in these methods, we also
%turn the above results into $(1-\epsilon)$-approximation algorithms trading off
%the running time, for appropriately dense random graphs as before. \vs

\noindent
{\bf New Models for Recommendation Supergraphs.}
Informed by our empirical experience, we identify the systematic ways in which real data sets diverge from the simple fixed-degree model under which we performed the analysis of the lazy engineer's methods. These lead us to define three new graph generation models for the candidate supergraph that allow for varying density between various categories of nodes on both sides: (i) To model sites where the nodes both sides of the graph are organized according to the same hierarchy,
we propose a {\em hierarchical tree model}, and extend our analysis: these hierarchies may come from topic ontologies in information sites such as Quora or news sites, or from product classifications in retail sites.
(ii) We then introduce a simple {\em cartesian product model} that partitions both sides into disjoint categories
and allows varying edge density across different pairs of
categories. (iii) To model the effect of strength of a candidate recommendation, we postulate a {\em weighted graph model} where all edges in the complete bipartite graph get recommendation weights that are identically distributed; we then examine when subgraphs with $L$-nodes picking a random set of $c$ edges come close to optimizing the number of nodes in $R$ that have total incoming weight of at least one. \vs

\noindent
{\bf Computational Results.} 
To validate our claims of effectiveness of a lazy engineer empirically, we ran several simulations for a range of values of $k (= \frac{l}{r})$ by varying the value of $c$ in each case. We are able to replicate the qualitative properties of our theoretical results for the random choice algorithm in Section~\ref{fixed-degree}. We also compare its performance against that of the greedy algorithm and a partition-based algorithm used to prove our results on perfect recommendation graphs. Our results conclusively show that it pays for the engineer to be a little less lazy than choosing a random set of recommendations, by being greedy in choosing the subgraph. This advantage persists for greedy even in practical scenarios with values of $a$ larger than one, a range where our results on random graph models do not show very strong guarantees. \vs


