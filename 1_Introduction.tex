\begin{abstract}

Recommendations are central to the utility of many of the popular websites such
as YouTube, Facebook and Quora, as well as many e-commerce store websites.
Such sites usually have a large list of candidate pages that could serve as
recommendations from the current page based on relevance. To inter-connect
the website for efficient traversal it is critical to subselect a few
recommendations on each page from the list of all candidates.

%If every page is a
%node, these candidate recommendations can be denoted as directed
%edges. We introduce the recommendation subgraph problem which
%informally attempts to choose a subgraph of bounded
%outdegree (denoting the limited page space to display the
%ecommendations) such that the resulting graph and the website
%can be traversed `efficiently.'
%In a generalization of the problem, 
In our formulation of the problem, the set of pages on the site is divided into a few popular
pages where surfers arrive and the remaining large number of pages
that should be recommended from the popular pages. The natural
bipartite graph of potential recommendations is a candidate
supergraph. Given the limited space to display recommendations a
subgraph that has bounded outdegree $c$ on the popular pages must be
chosen. Our goal is to maximize the number of undiscovered pages that have
indegree at least $a$. We introduce this as the $(c,
a)$-recommendation subgraph problem. 
%Maximizing the number of pages that are recommended by at least two or more popular pages leads to a much harder problem.

%We describe our work in solving such recommendation subgraph problems in practice at web scale. 
To solve such problems optimally at web-scale graph sizes,
implementations would involve writing distributed matching
algorithms. Instead, in this work, we study the effectiveness of a
lazy engineer solving the recommendation subgraph problem.  We
investigate the cases when the candidate supergraph is a random graph
under two models: the fixed-degree model where every node on the left
has exactly $d$ random neighbors on the right, as well as the standard
Erd\"os-Renyi model with expected degree $d$ on the left side.  We
show that for most reasonable parameters of the models, the lazy
engineer would have found solutions very close to optimal. We further
show the conditions under which a perfect recommendation subgraph, a
generalization of perfect matchings, exists. Lastly, to more
realistically model web graphs we propose generalizations of the
random graph models using topic taxonomies, varying subgraph edge
densities and edge weights that simulate traffic patterns. Surprisingly, the lazy
engineer would still have found solutions near optimal under different parameters.

%We then extend the fixed-degree random graph model in many directions: one where the
%pages are situated in a natural hierarchical taxonomy (such as a topic ontology in Quora, or a product hierarchy in a store catalog), another where varying %densities are allowed between different subgraphs, and a third where the recommendation edges are weighted (typically according to the traffic that the %particular link is likely to generate) and the degree constraints are on the weighted degree. In all these cases, we identify conditions under which the lazy %engineer is very effective. We conclude with web-scale experimental results that motivate and validate our proposed models, as well as compare the %effectiveness of various heuristic methods that the lazy engineer would have then thought up in his freed up time.

\iffalse
In this paper, we study the problem of graph recommendations as
variants of bipartite matching problems. We consider the problem
of solving such matching problems in practice at web-scale. To achieve
this we introduce several models to simulate underlying input graph
structures. We then analyze the conditions under which a random sample
of edges using constant memory already suffices to be a
'good' recommendation algorithm as opposed the cases when we may consider
the more classical and involved linear memory polynomial time algorithms.
We also show how to select the number of recommendations per item while
building a website so that there exists a 'perfect' graph recommendation.
\fi

\end{abstract}

\section{Introduction}

One of the great benefits of the web as a useful source of hyperlinked
information comes from the careful choices made in crafting the recommendations
that link a given page to closely related pages. Even though this advantage was
identified well before the www was in place by Bush~\cite{Bush45}, it continues
to persist even tday. Smooth site navigation is enabled by carefully chosen
recommendations to closely related pages. Thus, the presence of recommendations
is an integral feature of several popular websites that are known to have high
user engagement as reflected in long sessions. Examples range from 
entertainment sites like YouTube that recommend a set of videos on the right
for every video watched by a user, information sites like Quora that recommend
a set of related questions on every question page, to retail sites like Amazon
that recommend similar products on every product page. \vs

%While recommendations are important, they are implemented typically by finding
%related items to display at runtime. That is, when a user lands on an item the
%recommendation systems decide online the set of other items to display. Such
%systems have the problem that a global analysis of the performance of the
%recommendations are hard. E.g., it is almost impossible to know if there is an
%item that was not recommended by even one other item. As the number of
%pages of a site grows, doing such a real-time computation becomes less ideal 
%globally and might lead to deterioration in the link structure of the site.  This
%motivates the strategy of solving the recommendation problem off-line, where we
%pre-compute the set of recommendations for every item. 

In this work, we analyze a recommendation system that can be built
offline in two stages: the first decides on candidate recommendations
for each item based on relevance and retrieves a large candidate set
of $d$ items \cite{Schafer1999, Adomavicius2005, Resnick1997}. The
second stages prunes it to $c < d$ such that globally we ensure that
the resulting recommendation subgraph is `efficient' for traversal. 

%For
%instance, we might want to ensure that the resulting graph minimizes
%the number of items that was not recommended by any other item. \vs

We can represent this notion of recommendations by using a directed graph. A
vertex is simply an item and a directed edge $(u, v)$ is a recommendation from
$u$ to $v$. Under this graph model for the above problem, if $c=1$ and we
require the chosen recommendation subgraph be strongly connected then our
problem reduces to Hamilton cycle, an NP-complete problem \cite{CLRS2001}. \vs

\subsection{Bipartite Recommendation Subgraphs}

We can generalize the above graph recommendation problem by using a
directed bipartite graphs. We can use one partition $L$ to represent
the set of items for which we are required to suggest
recommendations. We can use the other partition $R$ to represent the
set of items that are potentially recommended. Note that a single item
can be represented in both $L$ and $R$ if needed.  Now the input to
the problem is this directed graph $G$ where each vertex has $d$
recommendations. The output required is a subgraph $G'$ where each
vertex in $L$ has $c < d$ recommendations. The simplest goal is to
minimize the number of vertices that have in-degree less than an
integer $a$. We call this the $(c, a)$-recommendation subgraph
problem.  Note that if $a=c=1$ this is simply the problem of maximum
bipartite matching ~\cite{LovaszPlummer1986}. If $a=1$ and $c > 1$,
we obtain a $b$-matching problem, that can be converted to the
usual bipartite matching problem. \cite{Gabow1983}

{\bf Implementation Issues} Over the past few years, the second and
third authors have implemented complex graph algorithms in
cutting-edge web-technology companies including Google, Facebook and
BloomReach. There are two key hurdles in making such graph algorithms
practical. The first is that the method used must be very simple to
implement, debug and deploy. The second is that the method must scale
gracefully with larger sizes. \vs

Matching algorithms require linear memory and super-linear run-time
neither of which scale well. Note that the Facebook graph has over a
billion vertices\cite{FacebookNodes} and hundreds of billions of edges\cite{}. Even an
e-commerce website with 100M product pages and 100 recommendation candidates per
product would require easily over 160GB in main memory to run these
algorithms without using graph compression techniques that add additional technical difficulties
in development and maintenance. In practice offline problem instances are solved by using
distributed computing such as map-reduce \cite{DeanGhemawat2004}. 
However, efficient map-reduce algorithms for graph problems
are notoriously difficult and we are back to building a complicated
system. \vs

% http://www.longtail.com/the_long_tail/faq/
% The long tail: Why the future of business is selling less of more
% Growth dynamics of the World-Wide Web. Nature - Long tail by page
% Analysis of WWW traffic in Cambodia and Ghana - Zipf by page
% Location and time do matter: A long tail study of website requests: http://public.csusm.edu/ckumar/research/Kumar-etal-online-location-time-DSS.pdf 
% shows % Zipf with beta=-1.365
%%

\subsection{Algorithms and Analyses}

{\bf The Lazy Engineer} These reasons motivated the authors to
investigate the ``lazy" approach of choosing a very simple (almost
any) set of recommendation to see if they would produce near optimal
solutions at least under realistic scenarios in practice.  In
practice, this would immediately imply that a very simple back-end
system can be built in many cases without the need for a complicated
algorithm. If the thresholds provided by our analysis are insufficient
in quality, then the designer can consider implementing the classical
algorithms. \vs

To be more specific about practical scenarios we briefly describe
input web graph characteristics to give an idea for the sizes of $L$,
$R$, $c$ and $a$ to expect. It is well known, and can be verified by
prior experiences that a small number of `head' pages contribute to a
significant amount of the traffic while a long tail of the remaining
pages contribute to the rest \cite{HubermanAdamic1999, DuDemmerBrewer2006}. For example
demonstrated by one of the prior measurements~\cite{KumarNorrisSun2009}
it is not unreasonable to expect that 50\% of the traffic is
contributed by less than 1\% (a few thousands) of the web pages and a
large number of tail pages (a few hundreds of thousands) contributing
the other half of the traffic. This implies that in practice $L$ can
be up to two orders of magnitude smaller than $R$.  By observing
recommendations of Quora, Amazon, YouTube and our work in building
these technologies in practice, typical values for $c$ range from 5 to
20 recommendations per page. Values of $a$ are the hardest to nail
down but one can think of it as something that can range from $1$ to
$10$ roughly.

{\bf Recommendation Graph Models} The setting for investigating the
effectiveness of the lazy engineer is provided by using a random graph
model for the recommendation supergraph. Since the lazy algorithm
involves choosing {\em any} set of $c$ recommendations, the natural
random graph model that permits analysis of this method is the {\em
  fixed-degree} model where every node on the left has recommendations
to a random subset of size exactly $d$ nodes in the right. This model is
similar to the another model called $d$-out, where vertices from both
sides of the bipartition send $d$ edges to the other side of the bipartition
\cite{FriezePittel2004}.

We study this model in Section~\ref{fixed-degree}. Our main result
identifies the range of parameters involving $c,a,l=|L|$ and $r =|R|$
where the lazy algorithm is very effective. In addition to showing
that it is a $(1-\frac1e)$- approximation algorithm in expectation, we
also get much better bounds for the expected performance for a wide
range of realistic parameters and also prove high probability bounds
on its performance. We extend the models later to more realistic
graphs including a hierarchical, cartesian and weighted graph
models. \vs

{\bf The Greedy Algorithm}
While the lazy algorithm chooses any set of $c$ recommendations, the natural
greedy algorithm will need some work: scanning the nodes on the right that must
be discovered, we look to see if there are $a$ neighbors from the left that have
not exhausted their bound of $c$ edges in the subgraph, and if so, use them to
add this node to the discovered set. We study this algorithm in Section
~\ref{greedy}. We show the easy result that Greedy gives a 
$\frac{1}{a+1}$-approximation, and also do an average case analysis. However, we
use the usual Erd\"os-Renyi model \cite{ErdosRenyi59}
rather than the fixed-degree model for this analysis.

In the subsequent Section~\ref{worst-vs-avg}, we compare and plot the worst-case
performance of Greedy against the average case expected performance of the random
or lazy algorithm, as a basis for our later computational comparisons in a
similar vein. \vs

{\bf Optimal Recommendation Subgraphs}
Finally we turn to the question of whether the maximum number of nodes allowed
by the degree constraints can be covered (at least $a$ times) by a recommendation
subgraph: we say that there is an perfect $(c,a)$-recommendation subgraph in this
case. Under the usual Erd\"os-Renyi model, we use existing results on the
existence of a perfect matching to characterize the edge probability over which
there exist such optimal recommendation subgraphs, by using a subset partitioning
method for the analysis. By relaxing the condition of optimality of matching (i.e.
perfect matchings) to disallowing short augmenting paths in these methods, we also
turn the above results into $(1-\epsilon)$-approximation algorithms trading off
the running time, for appropriately dense random graphs as before. \vs
